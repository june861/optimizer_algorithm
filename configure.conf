[ENV]
env_name = Catcher,Pixelcopter,Pong,PuckWorld
env_num = 50
capacity = 10000

[MONITOR]
wandb = False
tensorboard = False

[NETWORK]
layers = 3
hidden_dims = 128,128

[DQN]
# the max iter of training
max_train_steps = 500
# the max steps in one game
max_eposide_step = 500
# the evaluation frequency
evaluate_freq = 10
# learning freqecny in one game
learn_freq = 10
# the evaluation times
evaluate_times = 10
# learning rate
dqn_lr = 0.001
# discounted parameter
gamma = 0.9
epsilon = 0.4
epsilon_min = 0.001
epsilon_decay = 0.0001
# batch size
batch_size = 4096
mini_batch_size = 256
# use lr decay trick
use_lr_decay = True
# the frequency to update target network
update_target = 200

[PPO]
max_train_steps = 500  
per_batch_steps = 500  
evaluate_freq = 20  
save_freq = 20  
batch_size = 4096  
mini_batch_size = 512  
hidden_width = 64  
lr_a = 0.001  
lr_c = 0.0001  
gamma = 0.99  
lamda = 0.95  
epsilon = 0.2   
use_off_policy = False  
use_buffer = True  
use_gae = True  
grad_clip_param = 0.5  
use_adv_norm = True  
use_state_norm = True  
use_reward_norm = False  
use_reward_scaling = True  
entropy_coef = 0.01  
use_lr_decay = True  
use_grad_clip = True  
use_orthogonal_init = True  
set_adam_eps = True
use_ppo_clip = True  