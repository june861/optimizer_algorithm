{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pso.pso import PSO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dim = 4\n",
    "p_num = 20\n",
    "x_min, x_max = -30, 30\n",
    "v_min, v_max = -60, 60\n",
    "\n",
    "PSO_Algorithm = PSO(C1 = 2, \n",
    "                    C2 = 2, \n",
    "                    Omega = 1,\n",
    "                    dim = dim,\n",
    "                    p_num = p_num, \n",
    "                    x_min = x_min, \n",
    "                    x_max = x_max, \n",
    "                    v_min = v_min, \n",
    "                    v_max = v_max\n",
    "                )\n",
    "for par in PSO_Algorithm.particles:\n",
    "    print(f'particle init pos is {par.get_pos()}')\n",
    "max_iter = 10000\n",
    "\n",
    "def cal_fitness(x):\n",
    "    return np.sum(x**2)\n",
    "    return sum(100.0 * (x[0][1:] - x[0][:-1] ** 2.0) ** 2.0 + (1 - x[0][:-1]) ** 2.0)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    print(f'iter is {i+1}, fitness is {cal_fitness(PSO_Algorithm.g_best)}')\n",
    "    PSO_Algorithm.update(cal_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSO_Algorithm.g_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation   #导入负责绘制动画的接口\n",
    "#其中需要输入一个更新数据的函数来为fig提供新的绘图信息\n",
    "\n",
    "fig, ax = plt.subplots()          #生成轴和fig,  可迭代的对象\n",
    "x, y= [], []    #用于接受后更新的数据\n",
    "line, = plt.plot([], [], '.-')   #绘制线对象，plot返回值类型，要加逗号\n",
    "\n",
    "#------说明--------#\n",
    "#核心函数包含两个：\n",
    "#一个是用于初始化画布的函数init()\n",
    "#另一个是用于更新数据做动态显示的update()\n",
    "\n",
    "\n",
    "def init():\n",
    "\t#初始化函数用于绘制一块干净的画布，为后续绘图做准备\n",
    "    ax.set_xlim(-5, 15*np.pi)    #初始函数，设置绘图范围\n",
    "    ax.set_ylim(-3, 3)\n",
    "    return line\n",
    "\n",
    "def update(step):           #通过帧数来不断更新新的数值\n",
    "    x.append(step)\n",
    "    y.append(np.cos(step/3)+np.sin(step**2))    #计算y\n",
    "    line.set_data(x, y)\n",
    "    return line\n",
    "\n",
    "#fig 是绘图的画布\n",
    "#update 为更新绘图的函数，step数值是从frames 传入\n",
    "#frames 数值是用于动画每一帧的数据\n",
    "ani = FuncAnimation(fig, update, frames=np.linspace(0, 13*np.pi, 128),\n",
    "                    init_func=init,interval=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(object):\n",
    "    def __init__(self,state_dim, act_dim, hidden_dims,layer_nums,train_params) -> None:\n",
    "        self.ppo_params = {\n",
    "            # 网络层参数\n",
    "            'state_dim' : state_dim,\n",
    "            'act_dim' : act_dim,\n",
    "            'layer_nums' : layer_nums,\n",
    "\n",
    "            # 训练参数\n",
    "            'lr_a': train_params['lr_a'],\n",
    "            'lr_c': train_params['lr_c'],\n",
    "            'batch_size' : train_params['batch_size'],\n",
    "            'on_policy' : False,\n",
    "            'use_buffer' : False,\n",
    "            'use_tanh' : train_params['use_tanh']\n",
    "        }\n",
    "\n",
    "        if not isinstance(hidden_dims,list) :\n",
    "            raise RuntimeError(f\"hidden_dims type must be list, now receive {type(hidden_dims)}. \")\n",
    "        if len(hidden_dims) != layer_nums - 1:\n",
    "            raise RuntimeError(f\"hidden_dims'len expect {layer_nums-1}, but now receive {len(hidden_dims)}. \")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "\n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "for index in BatchSampler(range(1024), 64, False):\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# from share_func import make_env\n",
    "# train_envs = [ make_env(env_name = \"CartPole-v1\", seed = 1,idx = i,capture_video = False, run_name = f'_video{i}') for i in range(1000) ]\n",
    "def make_env(gym_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        env = gym.make(gym_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "train_envs = [ make_env(gym_id = \"CartPole-v1\", seed = 1,idx = i,capture_video = False, run_name = f'_video{i}') for i in range(10) ]\n",
    "envs = gym.vector.SyncVectorEnv(train_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.make(\"CartPole-v1\")._max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.single_action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env reset, state shape is (env_num, state_dim)\n",
    "from ppo.ppo import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "ppo_params = {\n",
    "        # ppo algorithm params\n",
    "        'clip_param' : 0.2,\n",
    "\n",
    "        # 训练参数\n",
    "        'lr_a': 1e-3,\n",
    "        'lr_c': 1e-3,\n",
    "        'gamma': 0.5,\n",
    "        'lamda': 0.5,\n",
    "        'batch_size' : 1024,\n",
    "        'mini_batch_size': 64,\n",
    "\n",
    "        # trick params\n",
    "\n",
    "        'off_policy' : False, # use off-policy or on-policy\n",
    "        'use_buffer' : True, # use buffer to store or not\n",
    "        \"use_ppo_clip\":False , # use ppo clip param annealing\n",
    "        \"use_adv_norm\" : True, # use advantage normalization\n",
    "        \"use_state_norm\" : False, # use state normalization\n",
    "        \"use_reward_norm\" : False, # use reward normalization\n",
    "        'use_tanh' : False, # use tanh activate func or ReLU func\n",
    "        'use_adv_norm' : False, # use advantage normalization\n",
    "        'use_grad_clip' : True, # use grad clip in model params.\n",
    "        'grad_clip_params': 0.5,\n",
    "        'use_lr_decay': True,\n",
    "        'entropy_coef': 0.1,\n",
    "        'device': torch.device(\"cuda\"),        \n",
    "}\n",
    "agent = PPO(state_dim = 4,act_dim = 2, hidden_dims = [64,64], layer_nums = 3, train_params = ppo_params)\n",
    "state, _ = envs.reset()\n",
    "done = np.full((10,1), False)\n",
    "while np.sum(done) == 0:\n",
    "    print(state, np.sum(done),sep=\"\\t\\t\")\n",
    "    action, a_logprob = agent.select_action(state)\n",
    "    state_, reward, done, truncation, _ = envs.step(action)\n",
    "    # replay_buffer.add(state = state, action = action, reward = reward, next_state = state_, a_logprob = a_logprob, done = done)\n",
    "    if np.sum(done) == envs.action_space.nvec.shape[0] :\n",
    "        print(f'done is {done}')\n",
    "        print(f'state shape is {state_.shape}, \\nstate is {state_}')\n",
    "    state = state_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    " \n",
    "cap = cv2.VideoCapture('XXX.avi')  #返回一个capture对象\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES,50)  #设置要获取的帧号\n",
    "a,b=cap.read()  #read方法返回一个布尔值和一个视频帧。若帧读取成功，则返回True\n",
    "cv2.imshow('b', b)\n",
    "cv2.waitKey(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "t1 = torch.Tensor(np.ones((2,5)))\n",
    "t2 = torch.Tensor(np.zeros((2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1,2,3,4,5,6],[0,1,2,3,4,5]])\n",
    "b = torch.tensor([[5,4,3,2,1,0],[0,1,2,3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.gather(1,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 5., 4., 3., 2., 1.],\n",
       "        [0., 1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1., 2., 3., 4., 5., 6.]),\n",
       "indices=tensor([0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
